{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish-Soni08/CoRise-Deep_Learning_Essentials/blob/main/Week1_Student_Ashish_Soni.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 1 Project\n",
        "\n",
        "Welcome to the first week's project for Deep Learning Essentials!\n",
        "\n",
        "In this notebook, we will cover:\n",
        "\n",
        "1. Basics of PyTorch\n",
        "\n",
        "2. Creating a Neural Network\n",
        "\n",
        "3. Deep Learning Intuitions"
      ],
      "metadata": {
        "id": "g7y2K9enqnL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running this notebook, **please create a copy of this yourself so you can edit it as needed**. This can be done by just pressing \"Copy to Drive\", which is on the toolbar above, next to \"+ Code\" and \"+ Text\". \n",
        "\n",
        "Now, run the below cell to install the dependencies for this project:"
      ],
      "metadata": {
        "id": "IMwFG9mYaaT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq emoji torchviz"
      ],
      "metadata": {
        "id": "5xk2EN9BxjLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a85056b-d79a-414c-cc0c-5e4e379756b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñå                              | 10 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà                             | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñå                           | 30 kB 35.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 40 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 51 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 61 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 71 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 81 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 92 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 102 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 112 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 122 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 133 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 143 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 153 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 163 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 174 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 184 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 194 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 204 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 215 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216 kB 26.3 MB/s \n",
            "\u001b[?25h  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, mount the data required for this assignment, set up the project directory, and import the necessary libraries.\n",
        "\n",
        "Note: **You should select \"Add shortcut to Drive\" on the \"deep-learning-essentials\" folder to have it appear under \"/content/drive\".**"
      ],
      "metadata": {
        "id": "cnTt1QX3af04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PRtdlXDzuYly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1d89ea-9c9e-4eda-a029-cbf899d87f3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "#@markdown Specify the full \"deep-learning-essentials\" folder in your Google Drive. \n",
        "#@markdown You can find this by navigating in the folder icon tab on the left.\n",
        "directory = '/content/drive/MyDrive/DLE-Projects/' #@param {type:\"string\"}\n",
        "\n",
        "sys.path.append(directory)\n",
        "sys.path.append(os.path.join(directory, 'utils'))\n",
        "os.chdir(os.path.join(directory, 'Week1'))\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchviz import make_dot\n",
        "import matplotlib.pyplot as plt\n",
        "from assignment_helpers import check_answer"
      ],
      "metadata": {
        "id": "IQO_AgJEqFL_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Basics of PyTorch\n",
        "\n",
        "While deep learning can be expressed in highly mathematical terms, I like to think of it as a type of computer programming.<mark> Each layer is a function that takes inputs and returns outputs.</mark> Models are APIs which can return pre-defined data structures, and they can be trained using objects called Trainers.\n",
        "\n",
        "For deep learning, we have a very useful Python library called PyTorch. While there are other deep learning libraries such as Tensorflow, Theano, CNTK, etc. we will focus on PyTorch simply because it is the most popular.\n",
        "\n",
        "In many ways, to know Deep Learning is to know PyTorch, and vice-versa. As such, we'll start this project with a primer on PyTorch!"
      ],
      "metadata": {
        "id": "5PtlOGaxrRL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PyTorch Tensor\n",
        "\n",
        "PyTorch has its own classes for arrays and data, similar to how Numpy has objects like ```np.array```. In order to perform PyTorch operations, we just to convert data into a PyTorch Tensor (```torch.Tensor```).\n",
        "\n",
        "\n",
        "For reference, PyTorch tensors are covered in the following documenation: https://pytorch.org/docs/stable/tensors.html\n",
        "\n",
        "In the cell below, we do a few mathematical computations using PyTorch:"
      ],
      "metadata": {
        "id": "26Y7otbbmxSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch tensor with value 5 and another with value 7\n",
        "A = torch.tensor(5)\n",
        "B = torch.tensor(7)\n",
        "\n",
        "# Evaluate the following expression: C = A(A+B^2)+(A-B)^B\n",
        "C = A * (A+(B**2))+(A-B)**B\n",
        "\n",
        "# Print C\n",
        "print(C)\n",
        "\n",
        "check_answer(C.item() == 142)"
      ],
      "metadata": {
        "id": "OXBdgp-EcYKM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "594591ef-60de-4364-cbb9-3ae2a0b22f2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(142)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below, create a NumPy array (with random values) with dimensions (4x4) and convert it to a PyTorch tensor."
      ],
      "metadata": {
        "id": "RWGGeTssavBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HQEZciFopirj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ee066204-e379-45a8-b082-3063e1280eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy array: [[0.95595216 0.79585306 0.29761771 0.54339312]\n",
            " [0.6141036  0.34143325 0.39568473 0.90686635]\n",
            " [0.19446779 0.45829421 0.09906138 0.42688381]\n",
            " [0.36511475 0.91624731 0.87804418 0.80437293]]\n",
            "\n",
            "Torch tensor: tensor([[0.9560, 0.7959, 0.2976, 0.5434],\n",
            "        [0.6141, 0.3414, 0.3957, 0.9069],\n",
            "        [0.1945, 0.4583, 0.0991, 0.4269],\n",
            "        [0.3651, 0.9162, 0.8780, 0.8044]], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "numpy_array = np.random.rand(4,4) # TODO: Implement here\n",
        "pt_tensor = torch.from_numpy(numpy_array) # TODO: Implement here\n",
        "\n",
        "print(f\"Numpy array: {numpy_array}\")\n",
        "print()\n",
        "print(f\"Torch tensor: {pt_tensor}\")\n",
        "\n",
        "check_answer(type(pt_tensor)==torch.Tensor and numpy_array.shape==(4, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch tensors have many attributes. Here are a few useful ones:\n",
        "\n",
        "1. View the shape of tensors using ```.size()```.\n",
        "\n",
        "2. Reshape PyTorch tensors easily using  ```.view()```.\n",
        "\n",
        "3. Convert them to GPU-enabled tensors using ```.cuda()``` and back to CPU using ```.cpu()```. (For this to work, you'll have to go to Runtime > Change Runtime Type > Hardware Accelerator > GPU)\n",
        "\n",
        "4. Convert them back to numpy using ```.numpy()```.\n",
        "\n",
        "In the cell below, try each command:"
      ],
      "metadata": {
        "id": "_1qtkUv0xXEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print size of pt_tensor\n",
        "print(f\"Shape of Tensor: {pt_tensor.size()}\")\n",
        "\n",
        "# Reshape pt_tensor to (16,)\n",
        "pt_tensor = pt_tensor.view(16, )\n",
        "print(pt_tensor)\n",
        "print(f\"Before: {pt_tensor.dtype}\")\n",
        "\n",
        "# Change data type from int to float -- at first it didn't work unless we changed the datatype\n",
        "pt_tensor = pt_tensor.float()\n",
        "print(f\"After: {pt_tensor.dtype}\")\n",
        "\n",
        "# Make pt_tensor GPU-enabled\n",
        "if torch.cuda.is_available():\n",
        "  pt_tensor = pt_tensor.cuda()\n",
        "\n",
        "# No to-do here, these lines just pass the tensor through a GPU-enabled layer. \n",
        "pt_layer = nn.Linear(16, 10).cuda()\n",
        "layer_output = pt_layer(pt_tensor)\n",
        "\n",
        "# Finally, convert the output back into a CPU-backed numpy array:\n",
        "numpy_output = layer_output.cpu()\n",
        "\n",
        "check_answer(numpy_output.shape==(10,))"
      ],
      "metadata": {
        "id": "sGoFaC8RMzRQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "63ec2270-d673-49a2-d11e-4044a90b214d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Tensor: torch.Size([4, 4])\n",
            "tensor([0.9560, 0.7959, 0.2976, 0.5434, 0.6141, 0.3414, 0.3957, 0.9069, 0.1945,\n",
            "        0.4583, 0.0991, 0.4269, 0.3651, 0.9162, 0.8780, 0.8044],\n",
            "       dtype=torch.float64)\n",
            "Before: torch.float64\n",
            "After: torch.float32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see from above, ```nn.Linear``` creates a simple fully-connected layer. This means all the inputs to the layer are connected to all the outputs.\n",
        "\n",
        "The image below is a schematic of ```pt_layer```. Each circle represents one feature, and each line is a weight.\n",
        "\n",
        "Quick quiz: How many weights does ```pt_layer``` have?\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=10xA7A_O-RHv_Y0ertE2K11nukiE3qKJa\" width=\"600\">"
      ],
      "metadata": {
        "id": "GBdURAvE8bFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put your answer here!\n",
        "\n",
        "num_weights = 160\n",
        "\n",
        "check_answer(num_weights==np.multiply(*pt_layer.weight.size()))"
      ],
      "metadata": {
        "id": "g7BPbMf2w4UQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "dc71940a-2c22-4ef5-b94a-1b6413b4071d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! In a fully connected layer, every input feature is connected to every output feature (hence the name).\n",
        "\n",
        "Later in this course, we'll go into layers which are not fully connected (eg. convolutional).\n",
        "\n",
        "Another point to <mark>note here is that in the case where the number of layers equals 1, a neural network is really just a linear model (like linear regression or logistic regression).</mark>"
      ],
      "metadata": {
        "id": "cyZuOEaesdmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Layers\n",
        "\n",
        "The great thing about using PyTorch is that passing inputs through multiple layers is very straightforward -- <mark>we can just treat each layer like a function and \"chain\" the outputs.</mark>\n",
        "\n",
        "In the cell below, create another ```nn.Linear``` layer with dimensions such that the result of feeding ```pt.tensor``` through both layers results in a tensor of size 1."
      ],
      "metadata": {
        "id": "BKMoziqQ-Iks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create second_layer and \"feed\" pt_tensor through pt_layer and second_layer.\n",
        "# Remember, GPU-enabled layers only accept inputs from GPU-enabled layers!\n",
        "\n",
        "second_layer = nn.Linear(10,1).cuda()\n",
        "\n",
        "output = second_layer(layer_output)\n",
        "\n",
        "print(f'Outout:{output}')\n",
        "print(f'{second_layer.weight.size()}')\n",
        "\n",
        "\n",
        "check_answer(output.size()==torch.Size([1]))"
      ],
      "metadata": {
        "id": "uabNtkDfp5mo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "6e4489c4-d6b6-4ed1-b534-06bb48141b95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outout:tensor([0.2417], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([1, 10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Weights\n",
        "\n",
        "When we create an instance of ```nn.Linear```, the weights are randomly initialized. Through training, these weights become tuned to the task at hand.\n",
        "\n",
        "You can access the actual weights themselves using ```.weight```. The weights are PyTorch Parameters (object type ```torch.nn.parameter.Parameter```), which are essentially ```torch.Tensor``` objects with ```requires_grad=True```. The same is true of the bias, which is stored in ```.bias```.\n",
        "\n",
        "We're going to practice manually setting weights in a linear layer just so PyTorch layers do not feel so abstract.\n",
        "\n",
        "In the exercise below, we will be just setting all weights of ```pt_layer``` to ```1```s, and the bias to ```0```s. Then, we will pass through a tensor that is just integers, incremented from ```[1,...,16]```.\n",
        "\n",
        "What do you expect the output of this layer to be?\n",
        "\n",
        "Think this through for a second before implementing the code below."
      ],
      "metadata": {
        "id": "NTaBHfFT_p_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Playground\n",
        "\n",
        "### WEIGHTS ### \n",
        "print(f\"Weights of the layer -> {pt_layer.weight}\")\n",
        "print(pt_layer.weight.size())\n",
        "\n",
        "# pt_layer_weights_ones = torch.ones(pt_layer.weight.size())\n",
        "# print(pt_layer_weights_ones)\n",
        "# print(pt_layer_weights_ones.size())\n",
        "\n",
        "### BIAS ###\n",
        "\n",
        "print(f\"The biases of the layer -> {pt_layer.bias}\")\n",
        "print(pt_layer.bias.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVteoj0UZ2xI",
        "outputId": "18fbc46f-145a-4f0f-c921-12ddfb9a2df5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights of the layer -> Parameter containing:\n",
            "tensor([[ 0.1335,  0.1521, -0.0343,  0.2169, -0.1502,  0.1630,  0.0393, -0.1085,\n",
            "         -0.0655, -0.1870,  0.0524, -0.0184, -0.0216, -0.1905, -0.1098, -0.1431],\n",
            "        [-0.0118, -0.2329, -0.1780, -0.1028,  0.1942,  0.1609, -0.0349, -0.1712,\n",
            "         -0.0837, -0.0722,  0.0503, -0.0268, -0.2293,  0.1125,  0.2490,  0.0347],\n",
            "        [ 0.1297,  0.2429,  0.0084, -0.0671, -0.2057,  0.2047,  0.1830, -0.0097,\n",
            "         -0.2029,  0.1064,  0.1047, -0.2142, -0.0749,  0.1207, -0.2044, -0.0412],\n",
            "        [-0.0450, -0.0144,  0.0731,  0.1179, -0.0590, -0.1899, -0.1930, -0.2226,\n",
            "         -0.2347, -0.1343, -0.2097, -0.2085,  0.1302, -0.0494,  0.0547,  0.0917],\n",
            "        [ 0.1728, -0.1123, -0.2463, -0.2160, -0.0898, -0.2447, -0.1194, -0.1046,\n",
            "          0.0494,  0.2272, -0.2040, -0.2320, -0.1933, -0.0301, -0.0214, -0.2112],\n",
            "        [-0.2183,  0.2304, -0.1193, -0.0235,  0.0099,  0.2170,  0.1970,  0.1122,\n",
            "          0.0235, -0.1976,  0.2030, -0.2295, -0.1988,  0.0205, -0.1367, -0.0337],\n",
            "        [ 0.0204, -0.1390, -0.0681, -0.1036,  0.0589,  0.2399, -0.2119, -0.1069,\n",
            "         -0.2046, -0.0282,  0.1324,  0.1391,  0.2412, -0.0280, -0.1032,  0.1273],\n",
            "        [-0.1635,  0.2306, -0.0689, -0.1782,  0.2333, -0.0072,  0.1774,  0.1957,\n",
            "         -0.1292, -0.0507, -0.2193,  0.0188, -0.1888, -0.0538, -0.2161,  0.1016],\n",
            "        [ 0.0238, -0.0847, -0.1445, -0.2364,  0.2221, -0.1380,  0.0532,  0.1059,\n",
            "         -0.2324, -0.1404, -0.0494,  0.1364,  0.2413, -0.2283, -0.2107,  0.1227],\n",
            "        [ 0.0510, -0.1657,  0.1563,  0.1317, -0.0325, -0.1636, -0.1099, -0.2167,\n",
            "          0.0576, -0.1889,  0.1346, -0.2006, -0.1145, -0.1507, -0.1616, -0.0438]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([10, 16])\n",
            "The biases of the layer -> Parameter containing:\n",
            "tensor([ 0.1972, -0.1898,  0.0659,  0.1317,  0.2180,  0.1160,  0.1813,  0.2407,\n",
            "        -0.1123,  0.1224], device='cuda:0', requires_grad=True)\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our input tensor\n",
        "input_tensor = torch.Tensor(list(range(1, 17)))\n",
        "\n",
        "# Create a PyTorch tensor of ones that has the same shape as pt_layer.weight\n",
        "pt_layer_weights_ones = torch.ones(pt_layer.weight.size()) # Size -> [10,16]\n",
        "\n",
        "# Set the weights of pt_layer as pt_layer_weights_ones. (You will need to convert it to a PyTorch Parameter)\n",
        "pt_layer.weight = torch.nn.Parameter(pt_layer_weights_ones)\n",
        "\n",
        "# Similarly, set all the bias parameters of pt_layer as zero.\n",
        "pt_layer_bias_zeros = torch.zeros(pt_layer.bias.size())\n",
        "pt_layer.bias = torch.nn.Parameter(pt_layer_bias_zeros)\n",
        "\n",
        "# Finally, pass the result through pt_layer\n",
        "pt_layer_output = pt_layer(input_tensor)\n",
        "\n",
        "check_answer(pt_layer_output.sum().item()==sum(list(range(1, pt_layer.in_features+1)))*pt_layer.out_features)"
      ],
      "metadata": {
        "id": "3xsR8pDWp8qg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "b96e618e-adf5-42d4-e552-a49b17a07b8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you got this correct, the right answer is that the output is just a tensor of length 10 containing 136's!\n",
        "\n",
        "The basic idea is that in a linear layer with all ```1``` weights, and ```0``` bias, we are simply adding all the input values (which in this case is the sum of ```[1,...,16]```)."
      ],
      "metadata": {
        "id": "Ptq8ZnO5lO5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Gradients\n",
        "\n",
        "What is the purpose of the ```requires_grad=True``` attribute?\n",
        "\n",
        "While we can do math operations in PyTorch, <mark>it is mainly useful for its ability to automatically differentiate functions.</mark>\n",
        "\n",
        "Let's say we want the derivative of $f(x)$ at a given value of $x$, ie. $\\frac{d}{dx}f(x)|_{x}$.\n",
        "\n",
        "(Feel free to hop back onto the lecture pages in terms of why this is important to calculate!)\n",
        "\n",
        "For simple functions like $f(x) = x^2$, we know the analytical expression for $f'(x)$ is just $2x$. As such, evaluating the dervative at a point, like $x=5$ is just plug-and-play.\n",
        "\n",
        "When it comes to neural networks, $f'(x)$ is mostly intractable -- the layers are chained operations which can grow arbitrarily long.\n",
        "\n",
        "This is where autograd comes in. Given a function $f(x)$ which is computed by chaining many successive operations, it computes the derivative/gradients with respect to any $x$. \n",
        "\n",
        "Let's see a basic example of this in action. In the cell below, we compute the derivative of $f(x) = x ^ 2$ using autograd (No work for you to do there).\n",
        "\n",
        "This tutorial is very helpful for understanding autograd more: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
        "\n"
      ],
      "metadata": {
        "id": "WVffZc-eoEZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we set x = 5\n",
        "x = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "# Second, we evaluate y = x ^ 2\n",
        "y = x ** 2\n",
        "\n",
        "# Third, we use .backward() to compute the derivative\n",
        "y.backward()\n",
        "\n",
        "# Finally, we print out the derivative\n",
        "print(f'Derivative using formula: {2*x}, Autograd: {x.grad}')"
      ],
      "metadata": {
        "id": "xc9wE-0oTnnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee16cb5-8b70-4141-9d05-bb5e9ac63175"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Derivative using formula: 10.0, Autograd: 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To gain some appreciation for autograd, let's add some complexity to the function.\n",
        "\n",
        "Rather than just one operation, let's string together several operations on x:\n",
        "\n",
        "$f(x) = 3(4x+9((\\frac{x}{3})^2 + 2)))$\n",
        "\n",
        "In the cell below, rather than type out the entire equation, compute $f(x)$ by expanding out the order of operations, with one per line. Then compute the derivative at $x=5$."
      ],
      "metadata": {
        "id": "2HgqdXRhTrHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell, set x = 5 and compute f(x) using one line per operation.\n",
        "# Then call y.backward().\n",
        "\n",
        "x = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "### THIS\n",
        "# x1 = (x/3)\n",
        "# x2 = x1 ** 2\n",
        "# x3 = x2 + 2\n",
        "# x4 = x3 * 9\n",
        "# x5 = x4 + 4*x\n",
        "# y = x5 * 3\n",
        "\n",
        "# Keep adding x's until you finish!\n",
        "\n",
        "### OR THIS\n",
        "y = 3*(4*x + 9*(((x/3)**2) + 2))\n",
        "\n",
        "\n",
        "y.backward()\n",
        "\n",
        "print(f\"Autograd: {x.grad}\")\n",
        "check_answer(x.grad == 6*(x+2))"
      ],
      "metadata": {
        "id": "x6eSNRt7ZDdK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "52043cb5-2e1e-4ba5-85c4-27b04501558f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autograd: 42.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can verify that we got the right answer by getting the closed-form solution of $f'(x) = 6(x+2)$.\n",
        "\n",
        "You can visualize the graph that PyTorch generates implictly with all those operations:"
      ],
      "metadata": {
        "id": "C0y0pQZGZVxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The make_dot function of TorchViz makes visualizing graphs easy!\n",
        "make_dot(y)"
      ],
      "metadata": {
        "id": "Wym_yKq7Rqen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "b6b5a635-ad35-4825-f5a5-4d91db4640e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f6327a95f10>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"204pt\" height=\"491pt\"\n viewBox=\"0.00 0.00 204.00 491.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 487)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-487 200,-487 200,4 -4,4\"/>\n<!-- 140063843830288 -->\n<g id=\"node1\" class=\"node\">\n<title>140063843830288</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"124.5,-31 70.5,-31 70.5,0 124.5,0 124.5,-31\"/>\n<text text-anchor=\"middle\" x=\"97.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140063843897552 -->\n<g id=\"node2\" class=\"node\">\n<title>140063843897552</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"142,-86 53,-86 53,-67 142,-67 142,-86\"/>\n<text text-anchor=\"middle\" x=\"97.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140063843897552&#45;&gt;140063843830288 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140063843897552&#45;&gt;140063843830288</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M97.5,-66.9688C97.5,-60.1289 97.5,-50.5621 97.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.0001,-41.3678 97.5,-31.3678 94.0001,-41.3678 101.0001,-41.3678\"/>\n</g>\n<!-- 140063843897872 -->\n<g id=\"node3\" class=\"node\">\n<title>140063843897872</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"142,-141 53,-141 53,-122 142,-122 142,-141\"/>\n<text text-anchor=\"middle\" x=\"97.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140063843897872&#45;&gt;140063843897552 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140063843897872&#45;&gt;140063843897552</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M97.5,-121.9197C97.5,-114.9083 97.5,-105.1442 97.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.0001,-96.3408 97.5,-86.3408 94.0001,-96.3409 101.0001,-96.3408\"/>\n</g>\n<!-- 140063843899152 -->\n<g id=\"node4\" class=\"node\">\n<title>140063843899152</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"89,-306 0,-306 0,-287 89,-287 89,-306\"/>\n<text text-anchor=\"middle\" x=\"44.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140063843899152&#45;&gt;140063843897872 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140063843899152&#45;&gt;140063843897872</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M47.1827,-286.8292C52.8975,-266.4099 66.8668,-217.4493 80.5,-177 83.4214,-168.3323 86.9191,-158.8412 89.9803,-150.7806\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"93.343,-151.7876 93.6679,-141.1978 86.81,-149.2736 93.343,-151.7876\"/>\n</g>\n<!-- 140063843900560 -->\n<g id=\"node5\" class=\"node\">\n<title>140063843900560</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"148,-416 47,-416 47,-397 148,-397 148,-416\"/>\n<text text-anchor=\"middle\" x=\"97.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140063843900560&#45;&gt;140063843899152 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140063843900560&#45;&gt;140063843899152</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M92.7966,-396.7382C84.0134,-378.5089 64.9802,-339.0062 53.6057,-315.3986\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.6109,-313.5724 49.1172,-306.0828 50.3048,-316.6109 56.6109,-313.5724\"/>\n</g>\n<!-- 140063843900368 -->\n<g id=\"node10\" class=\"node\">\n<title>140063843900368</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"187,-361 98,-361 98,-342 187,-342 187,-361\"/>\n<text text-anchor=\"middle\" x=\"142.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">DivBackward0</text>\n</g>\n<!-- 140063843900560&#45;&gt;140063843900368 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140063843900560&#45;&gt;140063843900368</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M105.3384,-396.9197C111.5849,-389.2851 120.5018,-378.3867 128.0352,-369.1792\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"130.8249,-371.2968 134.4484,-361.3408 125.4071,-366.8641 130.8249,-371.2968\"/>\n</g>\n<!-- 140065696181328 -->\n<g id=\"node6\" class=\"node\">\n<title>140065696181328</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"124.5,-483 70.5,-483 70.5,-452 124.5,-452 124.5,-483\"/>\n<text text-anchor=\"middle\" x=\"97.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140065696181328&#45;&gt;140063843900560 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140065696181328&#45;&gt;140063843900560</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M97.5,-451.791C97.5,-444.0249 97.5,-434.5706 97.5,-426.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.0001,-426.0647 97.5,-416.0648 94.0001,-426.0648 101.0001,-426.0647\"/>\n</g>\n<!-- 140063843900048 -->\n<g id=\"node7\" class=\"node\">\n<title>140063843900048</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"179,-196 90,-196 90,-177 179,-177 179,-196\"/>\n<text text-anchor=\"middle\" x=\"134.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140063843900048&#45;&gt;140063843897872 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140063843900048&#45;&gt;140063843897872</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M128.0551,-176.9197C123.0239,-169.4409 115.8857,-158.8301 109.7739,-149.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.606,-147.6844 104.1202,-141.3408 106.798,-151.5917 112.606,-147.6844\"/>\n</g>\n<!-- 140063843897424 -->\n<g id=\"node8\" class=\"node\">\n<title>140063843897424</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"187,-251 98,-251 98,-232 187,-232 187,-251\"/>\n<text text-anchor=\"middle\" x=\"142.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140063843897424&#45;&gt;140063843900048 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140063843897424&#45;&gt;140063843900048</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M141.1065,-231.9197C140.0867,-224.9083 138.6664,-215.1442 137.404,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.8344,-205.7329 135.9314,-196.3408 133.9073,-206.7405 140.8344,-205.7329\"/>\n</g>\n<!-- 140063843899728 -->\n<g id=\"node9\" class=\"node\">\n<title>140063843899728</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"196,-306 107,-306 107,-287 196,-287 196,-306\"/>\n<text text-anchor=\"middle\" x=\"151.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">PowBackward0</text>\n</g>\n<!-- 140063843899728&#45;&gt;140063843897424 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140063843899728&#45;&gt;140063843897424</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M149.9323,-286.9197C148.785,-279.9083 147.1872,-270.1442 145.767,-261.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"149.1793,-260.6443 144.1103,-251.3408 142.2712,-261.7748 149.1793,-260.6443\"/>\n</g>\n<!-- 140063843900368&#45;&gt;140063843899728 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140063843900368&#45;&gt;140063843899728</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M144.0677,-341.9197C145.215,-334.9083 146.8128,-325.1442 148.233,-316.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"151.7288,-316.7748 149.8897,-306.3408 144.8207,-315.6443 151.7288,-316.7748\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wait, what does ```.backward()``` do exactly?\n",
        "\n",
        "PyTorch is able to compute the derivative of functions like the one above by \"chaining\" the derivatives (aka using the Chain Rule).\n",
        "\n",
        "Let's say we have a simple function, $y = 5x^2$.\n",
        "\n",
        "Using the chain rule and setting $u = x^2$, we can write:\n",
        "\n",
        "$\\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dx}$\n",
        "\n",
        "We know that:\n",
        "\n",
        "$\\frac{dy}{du} = 5$, $\\frac{du}{dx} = 2x$\n",
        "\n",
        "Just multiply those together and get:\n",
        "\n",
        "$\\frac{dy}{dx} = 5(2x) = 10x$\n",
        "\n",
        "\n",
        "This is essentially how ```.backward()``` works: it computes $\\frac{dy}{dx}$ by computing $\\frac{dy}{du}$ and $\\frac{du}{dx}$ separately.\n",
        "\n",
        "For a more in-depth tutorial on how this works under the hood, visit https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html. (TL;DR: We compute the derivatives of all variables with respect to all other variables and collect them into a matrix called the Jacobian).\n",
        "\n",
        "Let's try actually coding this example in the cell below:\n",
        "\n",
        "Evaluate the derivative of $y$ at $x = 5$. \n",
        "\n",
        "Note: to save space, PyTorch usually deletes intermediate gradients after running ```.backward()```. To keep them, just call ```.retain_grad()``` on variables you want to save.\n",
        "\n"
      ],
      "metadata": {
        "id": "7sOoY552aBpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor x = 5\n",
        "x = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "# Compute y = 5x^2 as two separate lines\n",
        "u = x ** 2\n",
        "y = 5 * u\n",
        "\n",
        "# Retain the intermediate gradient\n",
        "u.retain_grad()\n",
        "\n",
        "# Run .backward()\n",
        "y.backward()\n",
        "\n",
        "# Print out the gradient of x and u\n",
        "print(f\"Gradient of x -> {x.grad}\")\n",
        "print(f\"Gradient of u -> {u.grad}\")\n",
        "\n",
        "check_answer(x.grad == 10*x and u.grad == 5)"
      ],
      "metadata": {
        "id": "Gi392VuXe67N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "a94efd4e-086f-4479-8ec5-74bcb044d845"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of x -> 50.0\n",
            "Gradient of u -> 5.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! As you can verify from above, ```u.grad``` corresponds to $\\frac{dy}{du}$, while ```x.grad``` corresponds to $\\frac{dy}{dx}$.\n",
        "\n",
        "### The Nitty Gritty of Gradients\n",
        "\n",
        "How are the gradients above used to train neural networks?\n",
        "\n",
        "In the most basic case, we can create a model with no hidden layers, such that $y = wx+b$, (a linear regression model). \n",
        "\n",
        "This is equivalent to a ```nn.Linear(1, 1)``` layer.\n",
        "\n",
        "During training, we want the gradients of $w$ and $b$. In order to train the network, we also add a loss function on top of the model. In total, our basic model \"function\" is:\n",
        "\n",
        "$f(x) = \\mathcal{L}(wx+b, y)$, where $\\mathcal{L}$ is the loss function, and $y$ is the true value of the output we want to learn.\n",
        "\n",
        "In the case of a loss function like mean-squared error, the function looks like:\n",
        "\n",
        "$$f(x) = \\frac{1}{N}\\sum_{i=1}^N(wx_i+b - y_i)^2$$\n",
        "\n",
        "In the cell below, construct a simple linear regression model (just one weight and one bias) with mean-squared error.\n",
        "\n",
        "Using the given $x$ and $y$ data points, compute the derivative of the loss, $\\mathcal{L}$, with respect to $w$ and $b$.\n",
        "\n",
        "Set $w=1$ and $b=0$.\n"
      ],
      "metadata": {
        "id": "eBVSUUIr0J4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is the provided x and y tensor\n",
        "np.random.seed(0)\n",
        "x = np.random.normal(0, 5, 30)\n",
        "y = np.abs(3 * x + 2)\n",
        "x = torch.tensor(x, requires_grad=True)\n",
        "y = torch.tensor(y, requires_grad=True)\n",
        "\n",
        "# Initialize parameters w and b\n",
        "w = torch.tensor(1., requires_grad=True)\n",
        "b = torch.tensor(0., requires_grad=True)\n",
        "\n",
        "# Compute y_hat using w and b\n",
        "y_hat = w*x + b\n",
        "\n",
        "# Compute the loss\n",
        "loss = (y_hat - y) ** 2\n",
        "loss = loss.sum() / len(x)\n",
        "\n",
        "# Use backward on loss to compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# Print the loss\n",
        "print(f\"loss -> {loss}\")\n",
        "\n",
        "# Print the gradient of L with respect to w and b\n",
        "print(f\"Gradient of L with respect to w -> {w.grad}\")\n",
        "print(f\"Gradient of L with respect to b -> {b.grad}\")\n",
        "\n",
        "check_answer(int(w.grad)==-45 and int(b.grad)==-24 and int(loss.item()) == 258)"
      ],
      "metadata": {
        "id": "boop1ZDRgvUM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "805717db-97f2-4457-9a03-de626d02088f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss -> 258.5468409351736\n",
            "Gradient of L with respect to w -> -45.32195281982422\n",
            "Gradient of L with respect to b -> -24.97298240661621\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! You've just computed gradients for a linear regression model!\n",
        "\n",
        "The loss is quite high, but that's not your fault -- the weights were off!\n",
        "\n",
        "As a next step, let's try to \"improve\" our model by changing the parameters $w$ and $b$ according to the gradients.\n",
        "\n",
        "You may remember that the formula for updating weights from Gradient Descent:\n",
        "\n",
        "$w_{new} = w_{old} - \\eta \\nabla L(w_{old})$\n",
        "\n",
        "To be precise, ```w.grad```$ = \\nabla L(w_{old})$ and ```b.grad``` = $\\nabla L(b_{old})$.\n",
        "\n",
        "Using this information and a learning rate $\\eta=0.01$, make updates to $w$ and $b$ and recompute the loss."
      ],
      "metadata": {
        "id": "lG3m2j8zK3sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set learning rate\n",
        "lr = 0.01\n",
        "\n",
        "# Update w and b\n",
        "w_new = w - lr * w.grad\n",
        "b_new = b - lr * b.grad\n",
        "\n",
        "# Compute the new y_hat\n",
        "y_hat_new = w_new*x + b_new\n",
        "\n",
        "# Compute the new loss\n",
        "loss_new = (y_hat_new - y) ** 2\n",
        "loss_new = loss_new.sum() / len(x)\n",
        "# Print the loss and gradients for w and b\n",
        "print(f\"loss -> {loss_new}\")\n",
        "\n",
        "print(f\"Gradient of L with respect to new w -> {w_new.grad}\")\n",
        "print(f\"Gradient of L with respect to new b -> {b_new.grad}\")\n",
        "\n",
        "check_answer(int(loss_new.item())==239)"
      ],
      "metadata": {
        "id": "BlQmwxCzjWc_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "717076f6-0d8f-4771-ab72-e94be6ac575e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss -> 239.35033123341225\n",
            "Gradient of L with respect to new w -> None\n",
            "Gradient of L with respect to new b -> None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
            "  return self._grad\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you've done this correctly, the loss should have gone down!\n",
        "\n",
        "In practice, this process is done iteratively, so that each step improves upon the next.\n",
        "\n",
        "Using a for-loop, update $w$ and $b$ 10 times using the same method as above.\n",
        "\n",
        "Note: Be careful when updating $w$ and $b$ -- you may need to do some extra work to get it working beyond just setting ```w = w - lr * w.grad```. As a hint, calling ```.item()``` returns the value of a tensor as a Python number. Furthermore, to turn on gradients for a tensor, you can just add ```.requires_grad_(True)``` to the end."
      ],
      "metadata": {
        "id": "sSdWmr1hjwYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set learning rate\n",
        "lr = 0.01\n",
        "\n",
        "# Initialize parameters w and b\n",
        "w = torch.tensor(1., requires_grad=True)\n",
        "b = torch.tensor(0., requires_grad=True)\n",
        "\n",
        "for step in range(10):\n",
        "\n",
        "  # Compute y_hat using w and b\n",
        "  y_hat = w*x + b\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = (y_hat -y) ** 2\n",
        "  loss = loss.sum()/len(x)\n",
        "\n",
        "  # Print the loss\n",
        "  print(loss)\n",
        "  # Compute gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Update w and b\n",
        "  w = w.item() - lr * w.grad\n",
        "  w = torch.tensor(w, requires_grad=True)\n",
        "  b = b.item() - lr * b.grad\n",
        "  b = torch.tensor(b, requires_grad=True)\n",
        "\n",
        "check_answer(int(loss.item()) == 205)"
      ],
      "metadata": {
        "id": "La9DjR-W8_AN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "0eb3d9cf-faa2-4c15-ec2b-e9694374d7ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(258.5468, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(239.3503, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(233.3294, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(228.7456, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(224.4383, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(220.2887, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(216.2806, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(212.4082, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(208.6668, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(205.0519, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats! If you got this through part, you've just trained a basic model using Stochastic Gradient Descent (SGD). The loss has dropped at each step, which means you've done the job correctly.\n",
        "\n",
        "However, it looks like the loss is still relatively high!\n",
        "\n",
        "This is in part due to the fact that the data is actually non-linear:\n",
        "\n",
        "$$y = abs(3x+2)$$\n",
        "\n",
        "This is a plot of what it looks like:\n"
      ],
      "metadata": {
        "id": "hs0DluJg6RoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting our function\n",
        "\n",
        "x_plot = x.detach().numpy()\n",
        "y_plot = y.detach().numpy()\n",
        "plt.scatter(x_plot, y_plot)\n",
        "plt.xlabel('x values')\n",
        "plt.ylabel('y values')\n",
        "plt.title('Plot of y as a function of x')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "24_IeTVDXG9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7d147f55-c2e8-45f4-f268-563c2b85c45b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our simple linear regression model does not work so well because it is just a linear model. In other words, there's no way we can fit a line through check-marked shaped data!\n",
        "\n",
        "### Finally... A neural network!\n",
        "\n",
        "Luckily, a neural network is meant to fit non-linear data and should help learn this data better.\n",
        "\n",
        "We start with the same linear regression model from before (adding subscripts to weights since we'll be adding more weights now).\n",
        "\n",
        "$f(x) = w_1x + b_1$\n",
        "\n",
        "Now, we'll apply a *non-linear activation function* to $w_1x + b_1$. There are a lot of potential ones, but for simplicity let's use ReLU, which stands for Rectified Linear Unit.\n",
        "\n",
        "The gist of ReLU (denoted as $\\sigma$) is that if the input is positive, then it just returns the input. If the input is negative, it returns 0.\n",
        "\n",
        "ReLU: $\\sigma(z) = z $ if $z >= 0$, else 0.\n",
        "\n",
        "So far, our equation looks like this:\n",
        "\n",
        "$f(x) = \\sigma(w_1x + b_1)$\n",
        "\n",
        "For the next layer, we'll be using the output of this function as the input of a linear regression:\n",
        "\n",
        "$f(x) = w_2(\\sigma(w_1^Tx + b_1)) + b_2$\n",
        "\n",
        "This is a whole lot of notation, but you just need to get the main idea: We're using the output of the linear regression (plus activation) as the input of another linear regression.\n",
        "\n",
        "This is the same thing you did above with ```nn.Linear```, except now in raw PyTorch.\n",
        "\n",
        "In the cell below, perform the same optimization you did with linear regression, but now just with a neural network.\n",
        "\n",
        "Start with $w_1 = w_2 = 1$, and $b_1 = b_2 = 0$.\n",
        "\n",
        "Also, start with learning rate = 0.001!"
      ],
      "metadata": {
        "id": "tU5ixtrsXHi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set learning rate\n",
        "lr = 0.001\n",
        "\n",
        "# Define the ReLU function\n",
        "def ReLU(z):\n",
        "  z[z < 0] = 0\n",
        "  return z\n",
        "\n",
        "# Initialize parameters w1, w2 and b1, b2\n",
        "w1 = torch.tensor(1., requires_grad=True)\n",
        "w2 = torch.tensor(1., requires_grad=True)\n",
        "\n",
        "b1 = torch.tensor(0., requires_grad=True)\n",
        "b2 = torch.tensor(0., requires_grad=True)\n",
        "\n",
        "for step in range(10):\n",
        "\n",
        "  # Compute y_hat using w and b\n",
        "  y_hat = w2 * (ReLU(w1 * x + b1)) + b2\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = (y_hat - y) ** 2\n",
        "  loss = loss.sum() / len(x)\n",
        "\n",
        "  # Print the loss\n",
        "  print(loss)\n",
        "\n",
        "  # Compute gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Update w and b\n",
        "  w1 = w1.item() - lr * w1.grad\n",
        "  w1 = torch.tensor(w1, requires_grad=True)\n",
        "  b1 = b1.item() - lr * b1.grad\n",
        "  b1 = torch.tensor(b1, requires_grad=True)\n",
        "  w2 = w2.item() - lr * w2.grad\n",
        "  w2 = torch.tensor(w2, requires_grad=True)\n",
        "  b2 = b2.item() - lr * b2.grad\n",
        "  b2 = torch.tensor(b2, requires_grad=True)\n",
        "\n",
        "check_answer(int(loss.item()) == 70)"
      ],
      "metadata": {
        "id": "_SIOpdlofRRG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "b95b2e0b-259f-46d6-db43-4a8f1ed05115"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(199.2902, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(172.6538, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(146.8816, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(123.8799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(105.1059, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(91.1554, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(81.7086, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(75.8394, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(72.4537, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "tensor(70.6109, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this part worked for you, congrats!! You've officially trained a neural network! You'll notice that the loss is quite a bit lower than just a simple linear model, and that's the exciting part!\n",
        "\n",
        "We hope you will trust us that this intuition-building exercise will pay off.\n",
        "\n",
        "**This marks the end of the project for this week!**\n",
        "\n",
        "üî•üî•üî• **Great job, and we'll see you next week, where we'll actually be training neural networks for some real-world tasks!!** üî•üî•üî•\n",
        "\n",
        "### Bonus\n",
        "\n",
        "There are lots of deep learning techniques you can use with even the most basic neural networks. We want you to try the following tweaks to the model you create above.\n",
        "\n",
        "1. What happens if you change the learning rate? Try 0.1 and 0.001. What happens to the loss? What do you think is driving the difference?\n",
        "\n",
        "2. What happens if you train for longer than 10 steps? (eg. 100, 1000) Can you plot the results with the x-axis as the number of steps, and y-axis as the loss?\n",
        "\n",
        "3. Can you implement an additional layer? How does it either help or hurt the performance of the model?\n",
        "\n"
      ],
      "metadata": {
        "id": "4tHl7xjU-qlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying Different Learning Rates and Step Size"
      ],
      "metadata": {
        "id": "UI-hee4yyNFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set learning rate\n",
        "lr_list = [0.01, 0.1]\n",
        "steps_list = [10, 100, 1000]\n",
        "\n",
        "for steps in steps_list:\n",
        "  print(f\"Training Using -> {steps} Steps\")\n",
        "  for lr in lr_list:\n",
        "    print(f\"Learning Rate -> {lr}\")\n",
        "\n",
        "    # Initialize parameters w1, w2 and b1, b2\n",
        "    w1 = torch.tensor(1., requires_grad=True)\n",
        "    w2 = torch.tensor(1., requires_grad=True)\n",
        "\n",
        "    b1 = torch.tensor(0., requires_grad=True)\n",
        "    b2 = torch.tensor(0., requires_grad=True)\n",
        "\n",
        "    for step in range(steps):\n",
        "      # Compute y_hat using w and b\n",
        "      y_hat = w2 * (ReLU(w1 * x + b1)) + b2\n",
        "      # Compute the loss\n",
        "      loss = (y_hat - y) ** 2\n",
        "      loss = loss.sum() / len(x)\n",
        "      # Print the loss\n",
        "      print(f\"Loss for Step Size -> {steps} with learning rate -> {lr} in Step number -> {step} is -> {loss}\")\n",
        "      # Compute gradients\n",
        "      loss.backward()\n",
        "      # Update w and b\n",
        "      w1 = w1.item() - lr * w1.grad\n",
        "      w1 = torch.tensor(w1, requires_grad=True)\n",
        "      b1 = b1.item() - lr * b1.grad\n",
        "      b1 = torch.tensor(b1, requires_grad=True)\n",
        "      w2 = w2.item() - lr * w2.grad\n",
        "      w2 = torch.tensor(w2, requires_grad=True)\n",
        "      b2 = b2.item() - lr * b2.grad\n",
        "      b2 = torch.tensor(b2, requires_grad=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUT7B6UbyNp5",
        "outputId": "3c42eb29-776f-42b5-f28d-8f7708c5ac61"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Using -> 10 Steps\n",
            "Learning Rate -> 0.01\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 0 is -> 199.29020278807863\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 1 is -> 115.39006414838514\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 2 is -> 266.2749809489008\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 3 is -> 77.71429366351632\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 4 is -> 111.1439878975148\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 5 is -> 252.68736061743226\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 6 is -> 71.01572891600757\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 7 is -> 90.94991484382756\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 8 is -> 185.519698900034\n",
            "Loss for Step Size -> 10 with learning rate -> 0.01 in Step number -> 9 is -> 94.48181196209325\n",
            "Learning Rate -> 0.1\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 0 is -> 199.29020278807863\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 1 is -> 584973.6961164692\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 2 is -> 7.303179295017051e+16\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 3 is -> 1.3760330278562182e+50\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 4 is -> 9.20406644448672e+149\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 5 is -> nan\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 6 is -> nan\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 7 is -> nan\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 8 is -> nan\n",
            "Loss for Step Size -> 10 with learning rate -> 0.1 in Step number -> 9 is -> nan\n",
            "Training Using -> 100 Steps\n",
            "Learning Rate -> 0.01\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 0 is -> 199.29020278807863\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 1 is -> 115.39006414838514\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 2 is -> 266.2749809489008\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 3 is -> 77.71429366351632\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 4 is -> 111.1439878975148\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 5 is -> 252.68736061743226\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 6 is -> 71.01572891600757\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 7 is -> 90.94991484382756\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 8 is -> 185.519698900034\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 9 is -> 94.48181196209325\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 10 is -> 199.14470298594722\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 11 is -> 73.29710301418137\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 12 is -> 115.32015589242282\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 13 is -> 147.99606115504938\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 14 is -> 297.07884790507956\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 15 is -> 276.4808151234218\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 16 is -> 196.76579265220377\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 17 is -> 58.61527774153046\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 18 is -> 61.42863103992102\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 19 is -> 71.28157415990964\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 20 is -> 116.34823596709778\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 21 is -> 118.65452531936502\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 22 is -> 248.9671968360031\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 23 is -> 140.56755673452926\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 24 is -> 87.75645801598067\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 25 is -> 174.7743178283145\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 26 is -> 57.056553700221116\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 27 is -> 64.39124558258196\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 28 is -> 81.34924241710993\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 29 is -> 154.25999925265467\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 30 is -> 65.2414900317038\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 31 is -> 98.32056822642124\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 32 is -> 103.34385923680664\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 33 is -> 209.59536172858125\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 34 is -> 80.43640722607287\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 35 is -> 96.7291939075292\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 36 is -> 192.67105016039537\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 37 is -> 62.58012264398983\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 38 is -> 76.41950907287955\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 39 is -> 137.1570151219452\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 40 is -> 63.75224494331186\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 41 is -> 95.68043262605916\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 42 is -> 89.57752126971576\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 43 is -> 172.00139458649255\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 44 is -> 52.314362830955005\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 45 is -> 55.629365941602444\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 46 is -> 68.51214118754064\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 47 is -> 80.96706261568325\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 48 is -> 148.31020861262925\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 49 is -> 51.05058466184466\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 50 is -> 54.101034064597535\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 51 is -> 60.30775742169221\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 52 is -> 85.4708961343618\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 53 is -> 82.86935680678515\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 54 is -> 151.67522385600938\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 55 is -> 48.84854937767083\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 56 is -> 48.79254745087656\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 57 is -> 48.78508007913786\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 58 is -> 48.926904866711766\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 59 is -> 49.42304893196006\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 60 is -> 51.189128380068155\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 61 is -> 54.89955489523211\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 62 is -> 69.02480380950563\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 63 is -> 76.05613550072785\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 64 is -> 132.34653159537496\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 65 is -> 51.07138305992695\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 66 is -> 57.503277524812816\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 67 is -> 65.1528576558994\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 68 is -> 101.22890743796539\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 69 is -> 66.82692175289147\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 70 is -> 105.96681529080958\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 71 is -> 62.64904034344184\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 72 is -> 93.68872074070448\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 73 is -> 68.35805665378173\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 74 is -> 109.89721239614778\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 75 is -> 57.949317914938256\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 76 is -> 79.64968915982806\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 77 is -> 71.14360637850713\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 78 is -> 116.77445249975898\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 79 is -> 52.35312258599096\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 80 is -> 63.17559535172598\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 81 is -> 67.24168418233364\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 82 is -> 105.87855543093742\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 83 is -> 56.5789218661628\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 84 is -> 75.76435506885187\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 85 is -> 68.42346084531232\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 86 is -> 108.54393431554412\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 87 is -> 53.522588826553985\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 88 is -> 67.0580143300365\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 89 is -> 66.66949354418233\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 90 is -> 103.50675660612939\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 91 is -> 54.776876289195386\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 92 is -> 70.74417145252173\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 93 is -> 66.11892933454156\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 94 is -> 101.65734971114468\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 95 is -> 54.362972015512426\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 96 is -> 69.6546006633238\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 97 is -> 64.98007890243728\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 98 is -> 98.3334736156273\n",
            "Loss for Step Size -> 100 with learning rate -> 0.01 in Step number -> 99 is -> 54.78424555431603\n",
            "Learning Rate -> 0.1\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 0 is -> 199.29020278807863\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 1 is -> 584973.6961164692\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 2 is -> 7.303179295017051e+16\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 3 is -> 1.3760330278562182e+50\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 4 is -> 9.20406644448672e+149\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 5 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 6 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 7 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 8 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 9 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 10 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 11 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 12 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 13 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 14 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 15 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 16 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 17 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 18 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 19 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 20 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 21 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 22 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 23 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 24 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 25 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 26 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 27 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 28 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 29 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 30 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 31 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 32 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 33 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 34 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 35 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 36 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 37 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 38 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 39 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 40 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 41 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 42 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 43 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 44 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 45 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 46 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 47 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 48 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 49 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 50 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 51 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 52 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 53 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 54 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 55 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 56 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 57 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 58 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 59 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 60 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 61 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 62 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 63 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 64 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 65 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 66 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 67 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 68 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 69 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 70 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 71 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 72 is -> nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 73 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 74 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 75 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 76 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 77 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 78 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 79 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 80 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 81 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 82 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 83 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 84 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 85 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 86 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 87 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 88 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 89 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 90 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 91 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 92 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 93 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 94 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 95 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 96 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 97 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 98 is -> nan\n",
            "Loss for Step Size -> 100 with learning rate -> 0.1 in Step number -> 99 is -> nan\n",
            "Training Using -> 1000 Steps\n",
            "Learning Rate -> 0.01\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 0 is -> 199.29020278807863\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 1 is -> 115.39006414838514\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 2 is -> 266.2749809489008\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 3 is -> 77.71429366351632\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 4 is -> 111.1439878975148\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 5 is -> 252.68736061743226\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 6 is -> 71.01572891600757\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 7 is -> 90.94991484382756\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 8 is -> 185.519698900034\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 9 is -> 94.48181196209325\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 10 is -> 199.14470298594722\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 11 is -> 73.29710301418137\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 12 is -> 115.32015589242282\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 13 is -> 147.99606115504938\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 14 is -> 297.07884790507956\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 15 is -> 276.4808151234218\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 16 is -> 196.76579265220377\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 17 is -> 58.61527774153046\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 18 is -> 61.42863103992102\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 19 is -> 71.28157415990964\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 20 is -> 116.34823596709778\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 21 is -> 118.65452531936502\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 22 is -> 248.9671968360031\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 23 is -> 140.56755673452926\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 24 is -> 87.75645801598067\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 25 is -> 174.7743178283145\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 26 is -> 57.056553700221116\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 27 is -> 64.39124558258196\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 28 is -> 81.34924241710993\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 29 is -> 154.25999925265467\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 30 is -> 65.2414900317038\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 31 is -> 98.32056822642124\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 32 is -> 103.34385923680664\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 33 is -> 209.59536172858125\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 34 is -> 80.43640722607287\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 35 is -> 96.7291939075292\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 36 is -> 192.67105016039537\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 37 is -> 62.58012264398983\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 38 is -> 76.41950907287955\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 39 is -> 137.1570151219452\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 40 is -> 63.75224494331186\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 41 is -> 95.68043262605916\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 42 is -> 89.57752126971576\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 43 is -> 172.00139458649255\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 44 is -> 52.314362830955005\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 45 is -> 55.629365941602444\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 46 is -> 68.51214118754064\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 47 is -> 80.96706261568325\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 48 is -> 148.31020861262925\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 49 is -> 51.05058466184466\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 50 is -> 54.101034064597535\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 51 is -> 60.30775742169221\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 52 is -> 85.4708961343618\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 53 is -> 82.86935680678515\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 54 is -> 151.67522385600938\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 55 is -> 48.84854937767083\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 56 is -> 48.79254745087656\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 57 is -> 48.78508007913786\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 58 is -> 48.926904866711766\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 59 is -> 49.42304893196006\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 60 is -> 51.189128380068155\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 61 is -> 54.89955489523211\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 62 is -> 69.02480380950563\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 63 is -> 76.05613550072785\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 64 is -> 132.34653159537496\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 65 is -> 51.07138305992695\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 66 is -> 57.503277524812816\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 67 is -> 65.1528576558994\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 68 is -> 101.22890743796539\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 69 is -> 66.82692175289147\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 70 is -> 105.96681529080958\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 71 is -> 62.64904034344184\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 72 is -> 93.68872074070448\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 73 is -> 68.35805665378173\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 74 is -> 109.89721239614778\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 75 is -> 57.949317914938256\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 76 is -> 79.64968915982806\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 77 is -> 71.14360637850713\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 78 is -> 116.77445249975898\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 79 is -> 52.35312258599096\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 80 is -> 63.17559535172598\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 81 is -> 67.24168418233364\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 82 is -> 105.87855543093742\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 83 is -> 56.5789218661628\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 84 is -> 75.76435506885187\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 85 is -> 68.42346084531232\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 86 is -> 108.54393431554412\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 87 is -> 53.522588826553985\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 88 is -> 67.0580143300365\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 89 is -> 66.66949354418233\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 90 is -> 103.50675660612939\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 91 is -> 54.776876289195386\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 92 is -> 70.74417145252173\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 93 is -> 66.11892933454156\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 94 is -> 101.65734971114468\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 95 is -> 54.362972015512426\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 96 is -> 69.6546006633238\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 97 is -> 64.98007890243728\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 98 is -> 98.3334736156273\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 99 is -> 54.78424555431603\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 100 is -> 70.86760561391118\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 101 is -> 63.93594789044527\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 102 is -> 95.30811861607953\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 103 is -> 55.090192392855506\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 104 is -> 71.72216511913429\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 105 is -> 62.96813821630716\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 106 is -> 92.5211015786261\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 107 is -> 55.30848602378391\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 108 is -> 72.30320614864577\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 109 is -> 61.92880878394703\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 110 is -> 89.60237656283898\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 111 is -> 55.61424470755612\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 112 is -> 73.08987268309934\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 113 is -> 60.88062486988227\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 114 is -> 86.70929539339583\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 115 is -> 55.91358504000173\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 116 is -> 73.8321678570927\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 117 is -> 59.85548956357526\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 118 is -> 83.92081915593585\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 119 is -> 56.160007964999295\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 120 is -> 74.4148329300127\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 121 is -> 58.88894273653221\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 122 is -> 81.32430409604358\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 123 is -> 56.31944120426565\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 124 is -> 74.5972620186189\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 125 is -> 58.068581909498725\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 126 is -> 78.9432137169322\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 127 is -> 56.398002714222386\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 128 is -> 74.67263136764656\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 129 is -> 57.336400235910695\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 130 is -> 76.95741355020921\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 131 is -> 56.3391137781778\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 132 is -> 74.40619182155632\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 133 is -> 56.743517421854605\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 134 is -> 75.35115703809905\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 135 is -> 56.1689327716341\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 136 is -> 73.87123479845344\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 137 is -> 56.26663911063409\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 138 is -> 74.05772089793739\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 139 is -> 55.927312314641874\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 140 is -> 73.00000763929712\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 141 is -> 55.92130524287756\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 142 is -> 72.91150952575104\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 143 is -> 55.6817562903183\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 144 is -> 72.25761388583892\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 145 is -> 55.58800563031825\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 146 is -> 71.96334833612777\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 147 is -> 55.41643492118789\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 148 is -> 71.48441702388092\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 149 is -> 55.29409983334636\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 150 is -> 71.12839430706559\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 151 is -> 55.150543832969646\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 152 is -> 70.72481110827835\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 153 is -> 55.0231979446762\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 154 is -> 70.36355326796632\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 155 is -> 54.89246632563061\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 156 is -> 69.99740731902742\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 157 is -> 54.76797899808094\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 158 is -> 69.64923777753349\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 159 is -> 54.644874978116086\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 160 is -> 69.3072832372491\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 161 is -> 54.52531433697424\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 162 is -> 68.97652974175601\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 163 is -> 54.40815030051536\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 164 is -> 68.65412326668971\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 165 is -> 54.293779052939904\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 166 is -> 68.34085509141181\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 167 is -> 54.181873319813306\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 168 is -> 68.03584730585075\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 169 is -> 54.07246537469798\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 170 is -> 67.73905155501127\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 171 is -> 53.96543522209528\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 172 is -> 67.45005906771935\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 173 is -> 53.860725891835166\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 174 is -> 67.168622941417\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 175 is -> 53.75824707103306\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 176 is -> 66.89444635307196\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 177 is -> 53.657936796629166\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 178 is -> 66.62726117584405\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 179 is -> 53.55973775537528\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 180 is -> 66.36685214611762\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 181 is -> 53.46356703796011\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 182 is -> 66.1129355361625\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 183 is -> 53.36937845187901\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 184 is -> 65.86530698221979\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 185 is -> 53.27711009422158\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 186 is -> 65.62373575640221\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 187 is -> 53.18668989149726\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 188 is -> 65.38797808672233\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 189 is -> 53.09808908644261\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 190 is -> 65.15788241650154\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 191 is -> 53.01123708975627\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 192 is -> 64.9332300417702\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 193 is -> 52.92607348316819\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 194 is -> 64.71379982506916\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 195 is -> 52.84256788474693\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 196 is -> 64.49944692191752\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 197 is -> 52.76067291907495\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 198 is -> 64.2899928165215\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 199 is -> 52.68031898205101\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 200 is -> 64.08524127042213\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 201 is -> 52.60148709948696\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 202 is -> 63.88508334731742\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 203 is -> 52.59597348619353\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 204 is -> 63.84564715376439\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 205 is -> 52.50035205834429\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 206 is -> 63.60612913669291\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 207 is -> 52.426376692044045\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 208 is -> 63.41764642102201\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 209 is -> 52.35085136224271\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 210 is -> 63.227568100289325\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 211 is -> 52.277930823386704\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 212 is -> 63.04491026820968\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 213 is -> 52.20658059655548\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 214 is -> 62.86710679718526\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 215 is -> 52.136770171929975\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 216 is -> 62.693929005250375\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 217 is -> 52.06838540414703\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 218 is -> 62.52498328194315\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 219 is -> 52.00133244028819\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 220 is -> 62.359978309704346\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 221 is -> 51.93552764686903\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 222 is -> 62.19902931809729\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 223 is -> 51.93264176396565\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 224 is -> 62.17264748637233\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 225 is -> 51.85635557274544\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 226 is -> 61.986320172667384\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 227 is -> 51.79266721558053\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 228 is -> 61.82975064315519\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 229 is -> 51.729475696132354\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 230 is -> 61.67579605019667\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 231 is -> 51.66807412110143\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 232 is -> 61.52705308523715\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 233 is -> 51.60805011162992\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 234 is -> 61.38239649419649\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 235 is -> 51.54930744465799\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 236 is -> 61.24147308116428\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 237 is -> 51.49172986922817\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 238 is -> 61.103898064458384\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 239 is -> 51.43522478170057\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 240 is -> 60.96939501348576\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 241 is -> 51.37975627959059\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 242 is -> 60.837819017787446\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 243 is -> 51.32525929468266\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 244 is -> 60.708974316520866\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 245 is -> 51.2716825636771\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 246 is -> 60.58268850096345\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 247 is -> 51.21900825826966\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 248 is -> 60.45889628553255\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 249 is -> 51.16719486420863\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 250 is -> 60.33745842532092\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 251 is -> 51.11623059387235\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 252 is -> 60.218329954340184\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 253 is -> 51.06606631747588\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 254 is -> 60.101379590428486\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 255 is -> 51.016687936743516\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 256 is -> 59.986539581371304\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 257 is -> 50.96809557180063\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 258 is -> 59.873800847467855\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 259 is -> 50.9202372453107\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 260 is -> 59.76301592641916\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 261 is -> 50.87312728511227\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 262 is -> 59.654212951308324\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 263 is -> 50.826714255559835\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 264 is -> 59.5472478849817\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 265 is -> 50.78100988690287\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 266 is -> 59.44212460889131\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 267 is -> 50.73598742873089\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 268 is -> 59.33877859900221\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 269 is -> 50.691622391647485\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 270 is -> 59.237141246871495\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 271 is -> 50.64791230039791\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 272 is -> 59.137191076941\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 273 is -> 50.60483196087493\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 274 is -> 59.038853539983755\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 275 is -> 50.56238389105223\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 276 is -> 58.94211816255918\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 277 is -> 50.52054043224063\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 278 is -> 58.846936616886886\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 279 is -> 50.47928900615474\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 280 is -> 58.753238045295724\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 281 is -> 50.4386296940057\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 282 is -> 58.66104141366921\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 283 is -> 50.39852970263875\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 284 is -> 58.570243360083346\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 285 is -> 50.35898816388151\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 286 is -> 58.48084162354829\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 287 is -> 50.32000383956229\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 288 is -> 58.39281901324741\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 289 is -> 50.281554030715775\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 290 is -> 58.3061192348421\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 291 is -> 50.24363137468205\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 292 is -> 58.220712898176664\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 293 is -> 50.206209581083854\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 294 is -> 58.13654399883485\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 295 is -> 50.16930443190309\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 296 is -> 58.05362347829126\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 297 is -> 50.13288836089622\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 298 is -> 57.971912766511124\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 299 is -> 50.0969642606416\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 300 is -> 57.89138797135915\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 301 is -> 50.06150803043631\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 302 is -> 57.81199207029615\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 303 is -> 50.02652277690928\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 304 is -> 57.733801351152486\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 305 is -> 50.00849633499272\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 306 is -> 57.68978229479594\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 307 is -> 49.97400062182986\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 308 is -> 57.611874578842034\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 309 is -> 49.93903329601308\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 310 is -> 57.53346154746505\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 311 is -> 49.90473937111209\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 312 is -> 57.45683243164965\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 313 is -> 49.87109799496542\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 314 is -> 57.381874918310885\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 315 is -> 49.838070310729215\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 316 is -> 57.30847488870054\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 317 is -> 49.80558964001895\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 318 is -> 57.23642348623341\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 319 is -> 49.77363083032825\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 320 is -> 57.16565466863889\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 321 is -> 49.742135679431364\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 322 is -> 57.096066309579356\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 323 is -> 49.71891995920205\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 324 is -> 57.04311751111979\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 325 is -> 49.68815173200877\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 326 is -> 56.97487937673814\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 327 is -> 49.65718066592292\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 328 is -> 56.90652952687718\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 329 is -> 49.626691172196836\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 330 is -> 56.839428410694424\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 331 is -> 49.59671439595271\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 332 is -> 56.77360533311594\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 333 is -> 49.56722306166007\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 334 is -> 56.7089651099219\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 335 is -> 49.5381895431825\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 336 is -> 56.64542527215733\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 337 is -> 49.51088127638766\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 338 is -> 56.43473645921404\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 339 is -> 49.47080226792393\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 340 is -> 56.3420649579343\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 341 is -> 49.440075278574014\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 342 is -> 56.27006704632508\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 343 is -> 49.412171253528754\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 344 is -> 56.20497955636708\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 345 is -> 49.38584966020313\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 346 is -> 56.14399973616756\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 347 is -> 49.36070910548669\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 348 is -> 56.08612340387758\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 349 is -> 49.33650207112026\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 350 is -> 56.0306888230554\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 351 is -> 49.313048482870734\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 352 is -> 55.97722883627723\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 353 is -> 49.29026079693513\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 354 is -> 55.92548286735957\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 355 is -> 49.26801034846985\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 356 is -> 55.875117912517005\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 357 is -> 49.246262344701925\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 358 is -> 55.826017266000115\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 359 is -> 49.224933561374534\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 360 is -> 55.77799396752115\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 361 is -> 49.20399356209885\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 362 is -> 55.730932813415585\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 363 is -> 49.18340357892809\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 364 is -> 55.684742676763335\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 365 is -> 49.163155533361866\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 366 is -> 55.6393903607637\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 367 is -> 49.14320675691047\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 368 is -> 55.594775447789196\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 369 is -> 49.12355481178997\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 370 is -> 55.55087154299825\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 371 is -> 49.104165766465506\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 372 is -> 55.5076232294185\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 373 is -> 49.08505827367837\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 374 is -> 55.465024270764985\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 375 is -> 49.066205110750644\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 376 is -> 55.42304866716085\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 377 is -> 49.04760888500345\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 378 is -> 55.381677830453405\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 379 is -> 49.02924012059844\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 380 is -> 55.34084458378165\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 381 is -> 49.01110294602839\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 382 is -> 55.3005652701241\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 383 is -> 48.99319486379081\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 384 is -> 55.26081567191384\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 385 is -> 48.9755155857774\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 386 is -> 55.22161001143988\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 387 is -> 48.9581176950485\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 388 is -> 55.05168977227144\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 389 is -> 48.9250385565412\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 390 is -> 54.97754941649079\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 391 is -> 48.90305539556702\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 392 is -> 54.926646088381\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 393 is -> 48.88457471894821\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 394 is -> 54.883541465773014\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 395 is -> 48.86771503498293\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 396 is -> 54.844290924492626\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 397 is -> 48.851879607305946\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 398 is -> 54.807557784099586\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 399 is -> 48.83680582969372\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 400 is -> 54.7727392960379\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 401 is -> 48.82234532551933\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 402 is -> 54.739453387188085\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 403 is -> 48.80840314510219\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 404 is -> 54.70745540504074\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 405 is -> 48.794881790421684\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 406 is -> 54.676502991333514\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 407 is -> 48.78169955815235\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 408 is -> 54.646408432859594\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 409 is -> 48.76883619343313\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 410 is -> 54.6170891434779\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 411 is -> 48.756250329031516\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 412 is -> 54.58845337630394\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 413 is -> 48.74390276705773\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 414 is -> 54.560407778291406\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 415 is -> 48.73177519323539\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 416 is -> 54.53289222425098\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 417 is -> 48.71984441166538\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 418 is -> 54.50585737731329\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 419 is -> 48.708094454165014\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 420 is -> 54.47923381226767\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 421 is -> 48.67720504693984\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 422 is -> 54.415777143996735\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 423 is -> 48.6618070949486\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 424 is -> 54.38288494473593\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 425 is -> 48.650032451870906\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 426 is -> 54.35708171035588\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 427 is -> 48.63910791702674\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 428 is -> 54.332882680728055\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 429 is -> 48.62836221439026\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 430 is -> 54.3090052699897\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 431 is -> 48.61768612977127\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 432 is -> 54.285235793437714\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 433 is -> 48.607046274610006\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 434 is -> 54.26153387014901\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 435 is -> 48.59645750795052\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 436 is -> 54.237930680259055\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 437 is -> 48.58592900993845\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 438 is -> 54.214454350817235\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 439 is -> 48.57547810570855\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 440 is -> 54.19115251679065\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 441 is -> 48.56511290461647\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 442 is -> 54.168044373438306\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 443 is -> 48.554835279876635\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 444 is -> 54.14513109284844\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 445 is -> 48.54465468449959\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 446 is -> 54.12243404884008\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 447 is -> 48.534569361775134\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 448 is -> 54.09995888861339\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 449 is -> 48.52457953923887\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 450 is -> 54.07769442303057\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 451 is -> 48.514702164877285\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 452 is -> 54.055690681309734\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 453 is -> 48.50491660120565\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 454 is -> 54.0338967862237\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 455 is -> 48.49523355807699\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 456 is -> 54.012327797147535\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 457 is -> 48.48564501465879\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 458 is -> 53.99098956500763\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 459 is -> 48.47617047997872\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 460 is -> 53.969904354589104\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 461 is -> 48.466775553740916\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 462 is -> 53.94900174110879\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 463 is -> 48.45750208611021\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 464 is -> 53.928367147974406\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 465 is -> 48.448312193482465\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 466 is -> 53.907925538368566\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 467 is -> 48.439222731597184\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 468 is -> 53.88770844972321\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 469 is -> 48.43022145678898\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 470 is -> 53.86769733363009\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 471 is -> 48.421325204648\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 472 is -> 53.84792386621863\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 473 is -> 48.41252175463483\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 474 is -> 53.82835288089628\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 475 is -> 48.403798929331195\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 476 is -> 53.80896774583455\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 477 is -> 48.39517356648886\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 478 is -> 53.78980470762401\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 479 is -> 48.386647322348786\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 480 is -> 53.77085508935318\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 481 is -> 48.378199678266505\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 482 is -> 53.75208923617939\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 483 is -> 48.36984208187135\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 484 is -> 53.73351761566855\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 485 is -> 48.36156433491108\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 486 is -> 53.71512739395537\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 487 is -> 48.35336483870212\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 488 is -> 53.696916304159224\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 489 is -> 48.345265037844015\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 490 is -> 53.67892443593265\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 491 is -> 48.3372350857011\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 492 is -> 53.661088152068075\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 493 is -> 48.32929448921598\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 494 is -> 53.643448135577536\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 495 is -> 48.32142989893328\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 496 is -> 53.62597167844873\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 497 is -> 48.31363471859017\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 498 is -> 53.608658704238145\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 499 is -> 48.30593190004304\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 500 is -> 53.59154322825728\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 501 is -> 48.298297118176734\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 502 is -> 53.574579042767155\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 503 is -> 48.29073132560743\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 504 is -> 53.55776607407038\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 505 is -> 48.283242469658994\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 506 is -> 53.54113257649092\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 507 is -> 48.27582263963922\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 508 is -> 53.52464301820069\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 509 is -> 48.2684899579817\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 510 is -> 53.5083445267496\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 511 is -> 48.26121135919946\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 512 is -> 53.492164991112936\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 513 is -> 48.254016466623135\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 514 is -> 53.476171929819124\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 515 is -> 48.246881894811494\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 516 is -> 53.46030620430735\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 517 is -> 48.23981613944752\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 518 is -> 53.44459900140132\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 519 is -> 48.23282231821613\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 520 is -> 53.42903591806567\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 521 is -> 48.225897417794855\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 522 is -> 53.413635849655066\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 523 is -> 48.21902135154375\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 524 is -> 53.398334422170386\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 525 is -> 48.21221702022036\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 526 is -> 53.38319959951391\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 527 is -> 48.20548141131658\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 528 is -> 53.368199965989625\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 529 is -> 48.198799297659\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 530 is -> 53.353332867879004\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 531 is -> 48.19219270979116\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 532 is -> 53.33861511310315\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 533 is -> 48.185639802684086\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 534 is -> 53.3240219077417\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 535 is -> 48.17914028126994\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 536 is -> 53.309544900113\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 537 is -> 48.17270262079428\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 538 is -> 53.295202967485125\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 539 is -> 48.16633343473274\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 540 is -> 53.28100453610285\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 541 is -> 48.16002152396674\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 542 is -> 53.26693445085372\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 543 is -> 48.15376000005593\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 544 is -> 53.2529757744785\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 545 is -> 48.14756210819909\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 546 is -> 53.23914764435729\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 547 is -> 48.14141667890379\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 548 is -> 53.22544066909336\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 549 is -> 48.135318179684354\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 550 is -> 53.21183368726648\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 551 is -> 48.129281975567274\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 552 is -> 53.19836492082511\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 553 is -> 48.1233043475433\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 554 is -> 53.18501123338489\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 555 is -> 48.11737235582577\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 556 is -> 53.17176378595023\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 557 is -> 48.11149923180337\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 558 is -> 53.15863434549261\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 559 is -> 48.10566517370742\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 560 is -> 53.145605935033664\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 561 is -> 48.0998916121357\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 562 is -> 53.132698994484116\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 563 is -> 48.09416745680163\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 564 is -> 53.11990228051756\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 565 is -> 48.08849723508073\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 566 is -> 53.107214088960546\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 567 is -> 48.08286464013538\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 568 is -> 53.094613862724025\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 569 is -> 48.077287638424636\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 570 is -> 53.08213186237635\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 571 is -> 48.071759941037456\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 572 is -> 53.06976286641783\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 573 is -> 48.066283443234056\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 574 is -> 53.05748989886702\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 575 is -> 48.060847930858266\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 576 is -> 53.045313934902104\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 577 is -> 48.055461874216434\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 578 is -> 53.033237130294154\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 579 is -> 48.05012161016469\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 580 is -> 53.02127235722028\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 581 is -> 48.04482776196896\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 582 is -> 53.00939793675014\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 583 is -> 48.03957882002376\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 584 is -> 52.997617190618904\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 585 is -> 48.03437585702624\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 586 is -> 52.98594440870435\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 587 is -> 48.029216093240485\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 588 is -> 52.97435910271396\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 589 is -> 48.02409278529036\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 590 is -> 52.962856133408806\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 591 is -> 48.0190209201027\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 592 is -> 52.951460646772674\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 593 is -> 48.01399118780094\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 594 is -> 52.94015903536237\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 595 is -> 48.00900599592717\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 596 is -> 52.92894684543576\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 597 is -> 48.00404532384386\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 598 is -> 52.91779676082691\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 599 is -> 47.999139555900086\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 600 is -> 52.90675860628859\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 601 is -> 47.99427518714663\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 602 is -> 52.89581502559766\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 603 is -> 47.98944945376913\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 604 is -> 52.88494899878618\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 605 is -> 47.984665583732436\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 606 is -> 52.87417614566616\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 607 is -> 47.97992730789302\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 608 is -> 52.86349973554776\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 609 is -> 47.97521932445909\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 610 is -> 52.8528858568559\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 611 is -> 47.97054671946393\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 612 is -> 52.842352462472796\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 613 is -> 47.96592002200451\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 614 is -> 52.831920003719475\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 615 is -> 47.96133176586992\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 616 is -> 52.82156426586328\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 617 is -> 47.95677192580923\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 618 is -> 52.81127402069596\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 619 is -> 47.95225493465294\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 620 is -> 52.80107353586642\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 621 is -> 47.947780629692744\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 622 is -> 52.790963356245356\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 623 is -> 47.94333294822237\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 624 is -> 52.78091243120174\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 625 is -> 47.93892508243996\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 626 is -> 52.770945967117854\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 627 is -> 47.93455042622192\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 628 is -> 52.76105783602849\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 629 is -> 47.930205456653916\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 630 is -> 52.75123096328288\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 631 is -> 47.92590068771029\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 632 is -> 52.74148343652993\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 633 is -> 47.921635145781146\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 634 is -> 52.7318303635097\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 635 is -> 47.91740695778914\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 636 is -> 52.72224653164616\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 637 is -> 47.913204007966\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 638 is -> 52.71272715578049\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 639 is -> 47.90903088351051\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 640 is -> 52.70327044338225\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 641 is -> 47.904891338232986\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 642 is -> 52.69388520064954\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 643 is -> 47.900788609351316\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 644 is -> 52.68458181432363\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 645 is -> 47.896724727493364\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 646 is -> 52.67535329870445\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 647 is -> 47.892691347063426\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 648 is -> 52.6661931579606\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 649 is -> 47.88868594218004\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 650 is -> 52.657104741516484\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 651 is -> 47.88470705580508\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 652 is -> 52.648072543107475\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 653 is -> 47.880777113934386\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 654 is -> 52.6391337892067\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 655 is -> 47.876858967384905\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 656 is -> 52.630227150004536\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 657 is -> 47.872983622931315\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 658 is -> 52.62141461800916\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 659 is -> 47.8691304495013\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 660 is -> 52.61264577150871\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 661 is -> 47.865310074888946\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 662 is -> 52.6039509312259\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 663 is -> 47.86151886962302\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 664 is -> 52.595317698294345\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 665 is -> 47.857758902150294\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 666 is -> 52.58674938212235\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 667 is -> 47.85402669620863\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 668 is -> 52.57825075759301\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 669 is -> 47.850329366598004\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 670 is -> 52.56981579729898\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 671 is -> 47.846656551902136\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 672 is -> 52.561439711455144\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 673 is -> 47.84300145363777\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 674 is -> 52.55310904281876\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 675 is -> 47.839386436354445\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 676 is -> 52.5448513129531\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 677 is -> 47.83578759214865\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 678 is -> 52.536640108145164\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 679 is -> 47.83223582661927\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 680 is -> 52.52852650383333\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 681 is -> 47.82869886834098\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 682 is -> 52.52044717411163\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 683 is -> 47.82518968713587\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 684 is -> 52.51242364988707\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 685 is -> 47.82170649989096\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 686 is -> 52.504465561887386\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 687 is -> 47.818245052030306\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 688 is -> 52.496548549595126\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 689 is -> 47.814819113368344\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 690 is -> 52.4887085102712\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 691 is -> 47.81141556232992\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 692 is -> 52.480916735405465\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 693 is -> 47.808041490165486\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 694 is -> 52.473191888103024\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 695 is -> 47.804689626326656\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 696 is -> 52.465512810538236\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 697 is -> 47.80136456418006\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 698 is -> 52.457898305347314\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 699 is -> 47.79806153687169\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 700 is -> 52.45032690906586\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 701 is -> 47.79479481461328\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 702 is -> 52.442833941426464\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 703 is -> 47.7915507885107\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 704 is -> 52.43538289099488\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 705 is -> 47.788319706039715\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 706 is -> 52.4279713406295\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 707 is -> 47.78511282779309\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 708 is -> 52.420610476713726\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 709 is -> 47.78193442650692\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 710 is -> 52.41331033418918\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 711 is -> 47.778778397705985\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 712 is -> 52.40606218418265\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 713 is -> 47.775653184428194\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 714 is -> 52.39887547101084\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 715 is -> 47.77254770062095\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 716 is -> 52.3917385452912\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 717 is -> 47.76946904176933\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 718 is -> 52.38465089574934\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 719 is -> 47.76640664061721\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 720 is -> 52.37760545768274\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 721 is -> 47.76336127053071\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 722 is -> 52.37060114215102\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 723 is -> 47.760347167745074\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 724 is -> 52.363663853153746\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 725 is -> 47.757356268706964\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 726 is -> 52.356773228572244\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 727 is -> 47.7543797132524\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 728 is -> 52.34991621620595\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 729 is -> 47.751427065400364\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 730 is -> 52.343108077417945\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 731 is -> 47.74850207184488\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 732 is -> 52.33636666856342\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 733 is -> 47.7455945306841\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 734 is -> 52.32966364493871\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 735 is -> 47.742712036020414\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 736 is -> 52.32301292655473\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 737 is -> 47.73985070210511\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 738 is -> 52.3164059885828\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 739 is -> 47.7370129242303\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 740 is -> 52.30984787178589\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 741 is -> 47.7341948190328\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 742 is -> 52.30333923749683\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 743 is -> 47.731391910188584\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 744 is -> 52.296864739500855\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 745 is -> 47.72861456041671\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 746 is -> 52.29044702533285\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 747 is -> 47.72585558974899\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 748 is -> 52.28406216584742\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 749 is -> 47.72311493857508\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 750 is -> 52.27772683524772\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 751 is -> 47.720392547685044\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 752 is -> 52.27142657099251\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 753 is -> 47.717688358268695\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 754 is -> 52.265168592666505\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 755 is -> 47.71501026533949\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 756 is -> 52.258961273765735\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 757 is -> 47.71234315138807\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 758 is -> 52.25279219639388\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 759 is -> 47.70970584720673\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 760 is -> 52.24667830848287\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 761 is -> 47.70708185649495\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 762 is -> 52.24059775439044\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 763 is -> 47.704485108043365\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 764 is -> 52.23457459674457\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 765 is -> 47.70190241085292\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 766 is -> 52.22858133052226\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 767 is -> 47.69933262735584\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 768 is -> 52.22262515858053\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 769 is -> 47.69678991942359\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 770 is -> 52.21671663858749\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 771 is -> 47.69425645550533\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 772 is -> 52.21083321425986\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 773 is -> 47.69174995573208\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 774 is -> 52.205010094477096\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 775 is -> 47.689249080205904\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 776 is -> 52.19920486679457\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 777 is -> 47.686772629281414\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 778 is -> 52.19345040721185\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 779 is -> 47.684319190552884\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 780 is -> 52.187748109736454\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 781 is -> 47.681878315354574\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 782 is -> 52.18206820669863\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 783 is -> 47.679453274567614\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 784 is -> 52.17643313398795\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 785 is -> 47.67705109472879\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 786 is -> 52.17083919763825\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 787 is -> 47.674655624538595\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 788 is -> 52.16526520059215\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 789 is -> 47.67229566015757\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 790 is -> 52.10037527360416\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 791 is -> 47.658016363443565\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 792 is -> 52.07226882789234\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 793 is -> 47.65109678878389\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 794 is -> 52.058194581593746\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 795 is -> 47.647098890130344\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 796 is -> 52.0497387847402\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 797 is -> 47.64431199950091\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 798 is -> 52.043643165123136\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 799 is -> 47.64208714347095\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 800 is -> 52.038674282866445\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 801 is -> 47.64016349122202\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 802 is -> 52.03431478495987\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 803 is -> 47.638402320550064\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 804 is -> 52.03029856543233\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 805 is -> 47.63676701158393\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 806 is -> 52.026553435854574\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 807 is -> 47.63520808746195\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 808 is -> 52.02297668494523\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 809 is -> 47.63371681451136\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 810 is -> 52.019544890890174\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 811 is -> 47.63226924312198\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 812 is -> 52.01621447651733\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 813 is -> 47.6308780297667\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 814 is -> 52.01300387860326\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 815 is -> 47.629510618648155\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 816 is -> 52.00984166816566\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 817 is -> 47.62817703245227\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 818 is -> 52.00675860114275\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 819 is -> 47.62686195955893\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 820 is -> 52.00371018607195\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 821 is -> 47.62556734182892\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 822 is -> 52.00071006229611\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 823 is -> 47.62428911287477\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 824 is -> 51.99774703713299\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 825 is -> 47.62302471729966\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 826 is -> 51.994811850680065\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 827 is -> 47.62177552765171\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 828 is -> 51.99190766029053\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 829 is -> 47.6205326316896\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 830 is -> 51.98901882133336\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 831 is -> 47.61930426831931\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 832 is -> 51.986150639075504\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 833 is -> 47.61807344958964\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 834 is -> 51.98328866537351\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 835 is -> 47.61685576187677\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 836 is -> 51.980454576481726\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 837 is -> 47.61564521032572\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 838 is -> 51.977633285507075\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 839 is -> 47.61444673930956\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 840 is -> 51.97483610691594\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 841 is -> 47.613246298640405\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 842 is -> 51.97203961973481\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 843 is -> 47.61205808536486\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 844 is -> 51.969262396798385\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 845 is -> 47.61087746138231\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 846 is -> 51.96650425051784\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 847 is -> 47.60969516059659\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 848 is -> 51.963740348327164\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 849 is -> 47.60852195348635\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 850 is -> 51.96099761375606\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 851 is -> 47.60735409752778\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 852 is -> 51.958262064286494\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 853 is -> 47.606186955607456\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 854 is -> 51.95553823242885\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 855 is -> 47.6050275102142\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 856 is -> 51.95281926360037\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 857 is -> 47.60386413333167\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 858 is -> 51.950101222274135\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 859 is -> 47.602714576368456\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 860 is -> 51.947401818359786\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 861 is -> 47.60157372569834\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 862 is -> 51.94472696280786\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 863 is -> 47.600438311192825\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 864 is -> 51.94206465810341\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 865 is -> 47.59929802808777\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 866 is -> 51.93938592464163\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 867 is -> 47.59815853458255\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 868 is -> 51.93671730693748\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 869 is -> 47.59702345881705\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 870 is -> 51.934053180910716\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 871 is -> 47.59590316964477\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 872 is -> 51.93142696784107\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 873 is -> 47.59478500698672\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 874 is -> 51.928802931943274\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 875 is -> 47.59367546899521\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 876 is -> 51.926187972918605\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 877 is -> 47.5925704427048\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 878 is -> 51.923588969170474\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 879 is -> 47.59146613904649\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 880 is -> 51.920992898375104\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 881 is -> 47.590361565703425\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 882 is -> 51.91839519895846\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 883 is -> 47.58926609724007\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 884 is -> 51.915818117071716\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 885 is -> 47.58817457282695\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 886 is -> 51.91324536213941\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 887 is -> 47.58709448155857\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 888 is -> 51.91070806712811\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 889 is -> 47.5860117291396\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 890 is -> 51.90815509146439\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 891 is -> 47.58493341411119\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 892 is -> 51.90561565571953\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 893 is -> 47.58387299751366\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 894 is -> 51.90311299390456\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 895 is -> 47.58280526987082\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 896 is -> 51.9005969083648\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 897 is -> 47.58174981814527\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 898 is -> 51.89810363884457\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 899 is -> 47.580696411002336\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 900 is -> 51.895620101490984\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 901 is -> 47.5796380181505\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 902 is -> 51.89312076798152\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 903 is -> 47.57858952239027\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 904 is -> 51.89065118693414\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 905 is -> 47.577550012285265\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 906 is -> 51.88819833226759\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 907 is -> 47.576518102137534\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 908 is -> 51.88575652559113\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 909 is -> 47.57548584366119\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 910 is -> 51.883315096596235\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 911 is -> 47.57445791926219\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 912 is -> 51.880887960344005\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 913 is -> 47.573439839525946\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 914 is -> 51.878481209930364\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 915 is -> 47.57242697409416\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 916 is -> 51.876080849342294\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 917 is -> 47.571411330602494\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 918 is -> 51.87367621218413\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 919 is -> 47.570400884937165\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 920 is -> 51.87128724849586\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 921 is -> 47.56939238404574\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 922 is -> 51.868911167959475\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 923 is -> 47.56839979178556\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 924 is -> 51.866557574381744\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 925 is -> 47.56740678381206\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 926 is -> 51.864204348045334\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 927 is -> 47.566423553664585\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 928 is -> 51.86186897094335\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 929 is -> 47.565445473211774\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 930 is -> 51.859552471621136\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 931 is -> 47.56446371183494\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 932 is -> 51.85722923649938\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 933 is -> 47.56348936322036\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 934 is -> 51.85491920545876\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 935 is -> 47.562529485046205\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 936 is -> 51.85264192989507\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 937 is -> 47.56155979861093\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 938 is -> 51.85034300946047\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 939 is -> 47.560601267311235\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 940 is -> 51.84807300824575\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 941 is -> 47.55964459594534\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 942 is -> 51.84579537597181\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 943 is -> 47.5586930152695\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 944 is -> 51.843543387727614\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 945 is -> 47.55775256285169\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 946 is -> 51.84130688033182\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 947 is -> 47.556814848525285\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 948 is -> 51.839082092675696\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 949 is -> 47.55588272633982\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 950 is -> 51.83686434590047\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 951 is -> 47.55495099333312\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 952 is -> 51.83465134979473\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 953 is -> 47.55403268851358\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 954 is -> 51.83246536391177\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 955 is -> 47.553107757968526\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 956 is -> 51.83027488223714\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 957 is -> 47.55219862444211\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 958 is -> 51.828109919140516\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 959 is -> 47.55128894718066\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 960 is -> 51.825946367606505\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 961 is -> 47.55038423860747\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 962 is -> 51.823792192054306\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 963 is -> 47.54948597162903\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 964 is -> 51.82165428197042\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 965 is -> 47.54858714154673\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 966 is -> 51.81951314992414\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 967 is -> 47.54769797236525\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 968 is -> 51.817399870956606\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 969 is -> 47.54680822622452\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 970 is -> 51.81528655181005\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 971 is -> 47.54592256387171\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 972 is -> 51.813177851291876\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 973 is -> 47.545051151667096\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 974 is -> 51.81109372722248\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 975 is -> 47.54417914224589\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 976 is -> 51.80901886747826\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 977 is -> 47.54331580786513\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 978 is -> 51.806954628939096\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 979 is -> 47.54244587220866\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 980 is -> 51.80488565677444\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 981 is -> 47.54158459881435\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 982 is -> 51.80283286090615\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 983 is -> 47.54073292536178\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 984 is -> 51.800804546088784\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 985 is -> 47.53988058519143\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 986 is -> 51.79876457860464\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 987 is -> 47.53903460103619\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 988 is -> 51.796752320502456\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 989 is -> 47.53819264189524\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 990 is -> 51.79474222791646\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 991 is -> 47.53735094363294\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 992 is -> 51.79273109999874\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 993 is -> 47.53652256832436\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 994 is -> 51.790752265224214\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 995 is -> 47.535691212149764\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 996 is -> 51.78877096726986\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 997 is -> 47.53486708508032\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 998 is -> 51.78680248876682\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.01 in Step number -> 999 is -> 47.53404695214005\n",
            "Learning Rate -> 0.1\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 0 is -> 199.29020278807863\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 1 is -> 584973.6961164692\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 2 is -> 7.303179295017051e+16\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 3 is -> 1.3760330278562182e+50\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 4 is -> 9.20406644448672e+149\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 5 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 6 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 7 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 8 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 9 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 10 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 11 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 12 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 13 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 14 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 15 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 16 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 17 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 18 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 19 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 20 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 21 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 22 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 23 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 24 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 25 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 26 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 27 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 28 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 29 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 30 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 31 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 32 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 33 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 34 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 35 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 36 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 37 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 38 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 39 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 40 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 41 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 42 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 43 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 44 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 45 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 46 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 47 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 48 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 49 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 50 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 51 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 52 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 53 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 54 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 55 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 56 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 57 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 58 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 59 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 60 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 61 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 62 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 63 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 64 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 65 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 66 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 67 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 68 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 69 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 70 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 71 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 72 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 73 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 74 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 75 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 76 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 77 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 78 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 79 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 80 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 81 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 82 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 83 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 84 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 85 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 86 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 87 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 88 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 89 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 90 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 91 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 92 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 93 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 94 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 95 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 96 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 97 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 98 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 99 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 100 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 101 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 102 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 103 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 104 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 105 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 106 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 107 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 108 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 109 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 110 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 111 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 112 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 113 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 114 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 115 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 116 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 117 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 118 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 119 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 120 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 121 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 122 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 123 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 124 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 125 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 126 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 127 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 128 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 129 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 130 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 131 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 132 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 133 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 134 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 135 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 136 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 137 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 138 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 139 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 140 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 141 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 142 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 143 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 144 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 145 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 146 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 147 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 148 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 149 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 150 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 151 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 152 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 153 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 154 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 155 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 156 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 157 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 158 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 159 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 160 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 161 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 162 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 163 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 164 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 165 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 166 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 167 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 168 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 169 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 170 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 171 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 172 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 173 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 174 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 175 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 176 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 177 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 178 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 179 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 180 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 181 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 182 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 183 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 184 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 185 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 186 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 187 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 188 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 189 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 190 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 191 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 192 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 193 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 194 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 195 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 196 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 197 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 198 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 199 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 200 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 201 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 202 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 203 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 204 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 205 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 206 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 207 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 208 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 209 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 210 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 211 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 212 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 213 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 214 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 215 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 216 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 217 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 218 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 219 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 220 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 221 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 222 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 223 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 224 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 225 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 226 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 227 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 228 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 229 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 230 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 231 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 232 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 233 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 234 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 235 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 236 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 237 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 238 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 239 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 240 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 241 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 242 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 243 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 244 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 245 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 246 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 247 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 248 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 249 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 250 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 251 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 252 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 253 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 254 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 255 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 256 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 257 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 258 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 259 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 260 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 261 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 262 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 263 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 264 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 265 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 266 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 267 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 268 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 269 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 270 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 271 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 272 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 273 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 274 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 275 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 276 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 277 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 278 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 279 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 280 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 281 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 282 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 283 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 284 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 285 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 286 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 287 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 288 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 289 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 290 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 291 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 292 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 293 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 294 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 295 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 296 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 297 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 298 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 299 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 300 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 301 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 302 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 303 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 304 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 305 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 306 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 307 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 308 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 309 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 310 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 311 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 312 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 313 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 314 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 315 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 316 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 317 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 318 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 319 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 320 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 321 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 322 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 323 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 324 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 325 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 326 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 327 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 328 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 329 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 330 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 331 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 332 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 333 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 334 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 335 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 336 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 337 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 338 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 339 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 340 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 341 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 342 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 343 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 344 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 345 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 346 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 347 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 348 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 349 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 350 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 351 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 352 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 353 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 354 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 355 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 356 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 357 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 358 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 359 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 360 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 361 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 362 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 363 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 364 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 365 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 366 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 367 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 368 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 369 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 370 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 371 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 372 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 373 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 374 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 375 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 376 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 377 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 378 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 379 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 380 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 381 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 382 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 383 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 384 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 385 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 386 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 387 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 388 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 389 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 390 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 391 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 392 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 393 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 394 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 395 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 396 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 397 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 398 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 399 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 400 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 401 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 402 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 403 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 404 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 405 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 406 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 407 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 408 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 409 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 410 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 411 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 412 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 413 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 414 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 415 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 416 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 417 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 418 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 419 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 420 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 421 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 422 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 423 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 424 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 425 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 426 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 427 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 428 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 429 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 430 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 431 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 432 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 433 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 434 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 435 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 436 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 437 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 438 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 439 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 440 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 441 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 442 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 443 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 444 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 445 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 446 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 447 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 448 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 449 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 450 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 451 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 452 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 453 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 454 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 455 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 456 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 457 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 458 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 459 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 460 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 461 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 462 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 463 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 464 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 465 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 466 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 467 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 468 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 469 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 470 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 471 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 472 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 473 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 474 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 475 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 476 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 477 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 478 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 479 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 480 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 481 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 482 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 483 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 484 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 485 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 486 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 487 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 488 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 489 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 490 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 491 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 492 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 493 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 494 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 495 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 496 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 497 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 498 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 499 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 500 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 501 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 502 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 503 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 504 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 505 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 506 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 507 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 508 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 509 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 510 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 511 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 512 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 513 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 514 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 515 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 516 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 517 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 518 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 519 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 520 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 521 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 522 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 523 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 524 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 525 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 526 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 527 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 528 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 529 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 530 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 531 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 532 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 533 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 534 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 535 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 536 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 537 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 538 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 539 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 540 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 541 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 542 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 543 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 544 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 545 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 546 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 547 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 548 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 549 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 550 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 551 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 552 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 553 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 554 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 555 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 556 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 557 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 558 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 559 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 560 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 561 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 562 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 563 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 564 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 565 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 566 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 567 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 568 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 569 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 570 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 571 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 572 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 573 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 574 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 575 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 576 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 577 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 578 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 579 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 580 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 581 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 582 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 583 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 584 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 585 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 586 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 587 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 588 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 589 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 590 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 591 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 592 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 593 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 594 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 595 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 596 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 597 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 598 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 599 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 600 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 601 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 602 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 603 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 604 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 605 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 606 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 607 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 608 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 609 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 610 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 611 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 612 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 613 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 614 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 615 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 616 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 617 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 618 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 619 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 620 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 621 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 622 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 623 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 624 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 625 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 626 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 627 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 628 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 629 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 630 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 631 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 632 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 633 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 634 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 635 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 636 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 637 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 638 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 639 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 640 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 641 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 642 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 643 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 644 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 645 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 646 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 647 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 648 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 649 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 650 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 651 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 652 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 653 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 654 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 655 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 656 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 657 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 658 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 659 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 660 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 661 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 662 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 663 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 664 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 665 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 666 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 667 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 668 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 669 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 670 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 671 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 672 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 673 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 674 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 675 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 676 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 677 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 678 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 679 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 680 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 681 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 682 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 683 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 684 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 685 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 686 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 687 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 688 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 689 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 690 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 691 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 692 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 693 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 694 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 695 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 696 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 697 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 698 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 699 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 700 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 701 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 702 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 703 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 704 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 705 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 706 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 707 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 708 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 709 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 710 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 711 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 712 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 713 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 714 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 715 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 716 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 717 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 718 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 719 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 720 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 721 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 722 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 723 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 724 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 725 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 726 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 727 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 728 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 729 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 730 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 731 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 732 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 733 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 734 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 735 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 736 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 737 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 738 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 739 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 740 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 741 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 742 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 743 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 744 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 745 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 746 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 747 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 748 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 749 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 750 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 751 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 752 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 753 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 754 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 755 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 756 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 757 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 758 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 759 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 760 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 761 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 762 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 763 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 764 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 765 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 766 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 767 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 768 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 769 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 770 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 771 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 772 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 773 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 774 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 775 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 776 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 777 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 778 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 779 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 780 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 781 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 782 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 783 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 784 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 785 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 786 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 787 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 788 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 789 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 790 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 791 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 792 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 793 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 794 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 795 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 796 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 797 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 798 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 799 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 800 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 801 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 802 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 803 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 804 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 805 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 806 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 807 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 808 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 809 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 810 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 811 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 812 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 813 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 814 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 815 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 816 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 817 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 818 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 819 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 820 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 821 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 822 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 823 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 824 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 825 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 826 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 827 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 828 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 829 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 830 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 831 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 832 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 833 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 834 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 835 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 836 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 837 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 838 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 839 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 840 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 841 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 842 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 843 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 844 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 845 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 846 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 847 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 848 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 849 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 850 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 851 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 852 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 853 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 854 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 855 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 856 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 857 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 858 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 859 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 860 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 861 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 862 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 863 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 864 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 865 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 866 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 867 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 868 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 869 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 870 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 871 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 872 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 873 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 874 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 875 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 876 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 877 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 878 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 879 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 880 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 881 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 882 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 883 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 884 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 885 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 886 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 887 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 888 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 889 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 890 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 891 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 892 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 893 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 894 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 895 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 896 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 897 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 898 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 899 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 900 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 901 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 902 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 903 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 904 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 905 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 906 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 907 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 908 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 909 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 910 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 911 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 912 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 913 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 914 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 915 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 916 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 917 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 918 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 919 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 920 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 921 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 922 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 923 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 924 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 925 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 926 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 927 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 928 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 929 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 930 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 931 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 932 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 933 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 934 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 935 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 936 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 937 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 938 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 939 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 940 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 941 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 942 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 943 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 944 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 945 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 946 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 947 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 948 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 949 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 950 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 951 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 952 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 953 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 954 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 955 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 956 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 957 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 958 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 959 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 960 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 961 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 962 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 963 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 964 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 965 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 966 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 967 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 968 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 969 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 970 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 971 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 972 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 973 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 974 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 975 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 976 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 977 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 978 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 979 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 980 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 981 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 982 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 983 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 984 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 985 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 986 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 987 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 988 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 989 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 990 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 991 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 992 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 993 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 994 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 995 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 996 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 997 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 998 is -> nan\n",
            "Loss for Step Size -> 1000 with learning rate -> 0.1 in Step number -> 999 is -> nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions:**\n",
        "\n",
        "| Sr.No | Learning Rate  | Step Size | Conclusion   |\n",
        "|------ |--------------  |---------- | -------------|\n",
        "| 1     | 0.01           | 10        | The loss increases and decreases in between the steps and the final loss is 94.48, the model struggles to achieve minimum loss value.           |\n",
        "| 2     | 0.1            | 10        | The model reaches a minimum loss value of 9.20 in just 5 steps.            |\n",
        "| 3     | 0.01           | 100       |  Minimum loss value of 54.78, the loss decreases for most steps with only certain steps showing increase in loss.            |\n",
        "| 4     | 0.1            | 100       | Minimum loss value in 9.20 in just 5 steps.          |\n",
        "| 5     | 0.01           | 1000      | A minimum loss value of 47.53, with loss steadily decreasing but fluctuations at certain steps.             |\n",
        "| 6     | 0.1            | 1000      | A loss value of 9.20 in just 5 steps.             |"
      ],
      "metadata": {
        "id": "QWgPCcX5yf_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Adding Another Layer"
      ],
      "metadata": {
        "id": "iVvOv3TZylAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set learning rate\n",
        "lr = 0.001\n",
        "\n",
        "# Initialize parameters w1, w2 and b1, b2\n",
        "w1 = torch.tensor(1., requires_grad=True)\n",
        "w2 = torch.tensor(1., requires_grad=True)\n",
        "w3 = torch.tensor(1., requires_grad=True)\n",
        "\n",
        "b1 = torch.tensor(0., requires_grad=True)\n",
        "b2 = torch.tensor(0., requires_grad=True)\n",
        "b3 = torch.tensor(0., requires_grad=True)\n",
        "\n",
        "for step in range(10):\n",
        "\n",
        "  # Compute y_hat using w and b\n",
        "  y_hat = w3 * (ReLU(w2 * (ReLU(w1 * x + b1)) + b2 )) + b3\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = (y_hat - y) ** 2\n",
        "  loss = loss.sum() / len(x)\n",
        "\n",
        "  # Print the loss\n",
        "  print(f\"Loss: {loss}\")\n",
        "\n",
        "  # Compute gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Update w and b\n",
        "  w1 = w1.item() - lr * w1.grad\n",
        "  w1 = torch.tensor(w1, requires_grad=True)\n",
        "  b1 = b1.item() - lr * b1.grad\n",
        "  b1 = torch.tensor(b1, requires_grad=True)\n",
        "  w2 = w2.item() - lr * w2.grad\n",
        "  w2 = torch.tensor(w2, requires_grad=True)\n",
        "  b2 = b2.item() - lr * b2.grad\n",
        "  b2 = torch.tensor(b2, requires_grad=True)\n",
        "  w3 = w3.item() - lr * w3.grad\n",
        "  w3 = torch.tensor(w3, requires_grad=True)\n",
        "  b3 = b3.item() - lr * b3.grad\n",
        "  b3 = torch.tensor(b3, requires_grad=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpUe_vqvyhHF",
        "outputId": "d8ab3d6c-db65-4c27-a766-82798f461bdd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 199.29020278807863\n",
            "Loss: 158.3164348783003\n",
            "Loss: 117.50552499281119\n",
            "Loss: 87.15605553151856\n",
            "Loss: 72.76940180538493\n",
            "Loss: 68.91660963107299\n",
            "Loss: 68.24297639744103\n",
            "Loss: 68.0497769703521\n",
            "Loss: 67.90351843447735\n",
            "Loss: 67.76191530573959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: \n",
        "\n",
        "By Adding another layer -> The loss steadily decreases, maybe we can say the network stabilises. So another layer helps the model peformance. "
      ],
      "metadata": {
        "id": "z4DNGLhXy1d-"
      }
    }
  ]
}